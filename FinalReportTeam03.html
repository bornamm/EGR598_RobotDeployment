<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">@import url('https://themes.googleusercontent.com/fonts/css?kit=QLQbO6TluYWZREPc-yIZk03-shGCly5trVCngVK7dBE');.lst-kix_ljgi7cf7oqdw-8>li:before{content:"" counter(lst-ctn-kix_ljgi7cf7oqdw-8,lower-roman) ". "}ol.lst-kix_ljgi7cf7oqdw-3.start{counter-reset:lst-ctn-kix_ljgi7cf7oqdw-3 0}.lst-kix_ljgi7cf7oqdw-1>li{counter-increment:lst-ctn-kix_ljgi7cf7oqdw-1}.lst-kix_ljgi7cf7oqdw-5>li:before{content:"" counter(lst-ctn-kix_ljgi7cf7oqdw-5,lower-roman) ". "}ol.lst-kix_ljgi7cf7oqdw-5.start{counter-reset:lst-ctn-kix_ljgi7cf7oqdw-5 0}.lst-kix_ljgi7cf7oqdw-6>li:before{content:"" counter(lst-ctn-kix_ljgi7cf7oqdw-6,decimal) ". "}.lst-kix_ljgi7cf7oqdw-7>li:before{content:"" counter(lst-ctn-kix_ljgi7cf7oqdw-7,lower-latin) ". "}ol.lst-kix_ljgi7cf7oqdw-2.start{counter-reset:lst-ctn-kix_ljgi7cf7oqdw-2 0}.lst-kix_ljgi7cf7oqdw-4>li{counter-increment:lst-ctn-kix_ljgi7cf7oqdw-4}.lst-kix_ljgi7cf7oqdw-7>li{counter-increment:lst-ctn-kix_ljgi7cf7oqdw-7}ol.lst-kix_ljgi7cf7oqdw-7.start{counter-reset:lst-ctn-kix_ljgi7cf7oqdw-7 0}ol.lst-kix_ljgi7cf7oqdw-6{list-style-type:none}ol.lst-kix_ljgi7cf7oqdw-4.start{counter-reset:lst-ctn-kix_ljgi7cf7oqdw-4 0}ol.lst-kix_ljgi7cf7oqdw-5{list-style-type:none}ol.lst-kix_ljgi7cf7oqdw-8{list-style-type:none}ol.lst-kix_ljgi7cf7oqdw-7{list-style-type:none}.lst-kix_ljgi7cf7oqdw-6>li{counter-increment:lst-ctn-kix_ljgi7cf7oqdw-6}.lst-kix_ljgi7cf7oqdw-0>li{counter-increment:lst-ctn-kix_ljgi7cf7oqdw-0}.lst-kix_ljgi7cf7oqdw-3>li{counter-increment:lst-ctn-kix_ljgi7cf7oqdw-3}.lst-kix_ljgi7cf7oqdw-8>li{counter-increment:lst-ctn-kix_ljgi7cf7oqdw-8}.lst-kix_ljgi7cf7oqdw-0>li:before{content:"" counter(lst-ctn-kix_ljgi7cf7oqdw-0,decimal) ". "}.lst-kix_ljgi7cf7oqdw-2>li{counter-increment:lst-ctn-kix_ljgi7cf7oqdw-2}.lst-kix_ljgi7cf7oqdw-5>li{counter-increment:lst-ctn-kix_ljgi7cf7oqdw-5}ol.lst-kix_ljgi7cf7oqdw-0.start{counter-reset:lst-ctn-kix_ljgi7cf7oqdw-0 0}ol.lst-kix_ljgi7cf7oqdw-1.start{counter-reset:lst-ctn-kix_ljgi7cf7oqdw-1 0}.lst-kix_ljgi7cf7oqdw-1>li:before{content:"" counter(lst-ctn-kix_ljgi7cf7oqdw-1,decimal) ". "}ol.lst-kix_ljgi7cf7oqdw-6.start{counter-reset:lst-ctn-kix_ljgi7cf7oqdw-6 0}ol.lst-kix_ljgi7cf7oqdw-2{list-style-type:none}.lst-kix_ljgi7cf7oqdw-4>li:before{content:"" counter(lst-ctn-kix_ljgi7cf7oqdw-4,lower-latin) ". "}ol.lst-kix_ljgi7cf7oqdw-8.start{counter-reset:lst-ctn-kix_ljgi7cf7oqdw-8 0}ol.lst-kix_ljgi7cf7oqdw-1{list-style-type:none}ol.lst-kix_ljgi7cf7oqdw-4{list-style-type:none}ol.lst-kix_ljgi7cf7oqdw-3{list-style-type:none}.lst-kix_ljgi7cf7oqdw-2>li:before{content:"" counter(lst-ctn-kix_ljgi7cf7oqdw-2,decimal) ". "}ol.lst-kix_ljgi7cf7oqdw-0{list-style-type:none}.lst-kix_ljgi7cf7oqdw-3>li:before{content:"" counter(lst-ctn-kix_ljgi7cf7oqdw-3,decimal) ". "}ol{margin:0;padding:0}table td,table th{padding:0}.c14{background-color:#ffff00;-webkit-text-decoration-skip:none;color:#000000;text-decoration:underline;vertical-align:baseline;text-decoration-skip-ink:none;font-size:14pt;font-family:"Times New Roman";font-style:normal}.c1{-webkit-text-decoration-skip:none;color:#000000;font-weight:400;text-decoration:underline;vertical-align:baseline;text-decoration-skip-ink:none;font-size:12pt;font-family:"Times New Roman";font-style:normal}.c0{-webkit-text-decoration-skip:none;color:#000000;font-weight:700;text-decoration:underline;vertical-align:baseline;text-decoration-skip-ink:none;font-size:16pt;font-family:"Times New Roman";font-style:normal}.c31{-webkit-text-decoration-skip:none;color:#000000;text-decoration:underline;vertical-align:baseline;text-decoration-skip-ink:none;font-size:12pt;font-family:"Times New Roman";font-style:normal}.c19{-webkit-text-decoration-skip:none;color:#000000;text-decoration:underline;vertical-align:baseline;text-decoration-skip-ink:none;font-size:14pt;font-family:"Times New Roman";font-style:normal}.c12{padding-top:0pt;padding-bottom:0pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:justify}.c3{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:14pt;font-family:"Times New Roman";font-style:normal}.c9{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:justify;height:11pt}.c5{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Times New Roman";font-style:normal}.c4{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left;height:11pt}.c8{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:9pt;font-family:"Times New Roman";font-style:italic}.c32{padding-top:0pt;padding-bottom:12pt;line-height:1.0;orphans:2;widows:2;text-align:left}.c26{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c16{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:justify}.c23{padding-top:0pt;padding-bottom:0pt;line-height:1.0;orphans:2;widows:2;text-align:justify}.c6{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:center}.c30{padding-top:0pt;padding-bottom:0pt;line-height:1.0;orphans:2;widows:2;text-align:left}.c17{font-size:9pt;font-family:"Times New Roman";font-style:italic;font-weight:400}.c22{-webkit-text-decoration-skip:none;color:#1155cc;text-decoration:underline;text-decoration-skip-ink:none}.c24{font-size:10pt;font-family:"Calibri";color:#4a86e8;font-weight:400}.c10{color:#000000;text-decoration:none;vertical-align:baseline;font-style:normal}.c20{font-weight:400;font-size:11pt;font-family:"Arial"}.c13{font-weight:400;font-size:11pt;font-family:"Times New Roman"}.c34{background-color:#ffffff;max-width:468pt;padding:72pt 72pt 72pt 72pt}.c2{font-size:12pt;font-family:"Ubuntu Mono";font-weight:400}.c7{font-size:12pt;font-family:"Times New Roman";font-weight:400}.c33{-webkit-text-decoration-skip:none;text-decoration:underline;text-decoration-skip-ink:none}.c18{font-weight:400;font-size:16pt;font-family:"Times New Roman"}.c21{color:inherit;text-decoration:inherit}.c35{color:#188038}.c29{page-break-after:avoid}.c11{font-weight:700}.c28{font-weight:400}.c15{height:11pt}.c25{font-style:italic}.c27{background-color:#ffff00}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:0pt;-webkit-text-decoration-skip:none;color:#000000;font-weight:700;text-decoration:underline;font-size:16pt;padding-bottom:0pt;line-height:1.15;page-break-after:avoid;text-decoration-skip-ink:none;font-family:"Times New Roman";orphans:2;widows:2;text-align:justify}h2{padding-top:0pt;color:#000000;font-weight:700;font-size:12pt;padding-bottom:0pt;font-family:"Times New Roman";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c34 doc-content"><p class="c6"><span class="c18">Project Update - Team 03</span></p><p class="c6"><span class="c7">Panth Patel, Roberto Siqueiros, Borna Mansoormoayad</span></p><p class="c6"><span class="c24"><a class="c21" href="https://www.google.com/url?q=https://github.com/bornamm/EGR598_RobotDeployment&amp;sa=D&amp;source=editors&amp;ust=1683185492623739&amp;usg=AOvVaw333tNJVyXUqeN7C6lrxA0V">Github Repository Link</a></span></p><h1 class="c12" id="h.7hsy7lt0cepb"><span class="c0">Introduction</span></h1><p class="c16"><span class="c7">The</span><span class="c7">&nbsp;project involves developing an object detection and tracking system using the OAK-D camera mounted on the Turtlebot platform. A custom dataset was created using the OAK-D camera and annotated using an online platform called Roboflow [2]. Roboflow is used to generate annotation files for training purposes. The data was augmented to improve the accuracy of the model. Afterwards, the object detection model was trained using the YOLOv5 </span><span class="c7">Nano architecture</span><span class="c7">. This lightweight neural network can run on embedded systems like Raspberry Pi. The trained model is capable of detecting multiple objects simultaneously. It provides information about the detection, such as the center point (x,y) of the bounding box, height and width, depth of the center point, class detected, and detection confidence. To incorporate the trained model into our Turtlebot platform, a ROS node was developed and called </span><span class="c2">custom_object_detection_node</span><span class="c7">, which extracts the bounding box attributes of the detected objects. This information is then published to </span><span class="c2">/object_center </span><span class="c7">topic, to which the </span><span class="c2">controller </span><span class="c7">node subscribes. The </span><span class="c2">controller</span><span class="c7">&nbsp;node reads the published commands to actuate the Turtlebot through the </span><span class="c2">/cmd_vel</span><span class="c7">&nbsp;topic. A PID controller was also integrated into the</span><span class="c2">&nbsp;controller</span><span class="c7">&nbsp;node to adjust the velocity of the Turtlebot based on the data received. In the initial objective, the depth was to be calculated using the attributes of the bounding box provided by the </span><span class="c2">custom_object_detection_node</span><span class="c7">&nbsp;node. To achieve this, a new ROS node was created and called </span><span class="c2">depth_extraction</span><span class="c7">, which subscribed to the </span><span class="c2">custom_object_detection_node</span><span class="c7">&nbsp;node. The </span><span class="c2">depth_extraction</span><span class="c7">&nbsp;node calculated the depth of the detected object using the stereo depth data obtained from the </span><span class="c2">/stereo/depth</span><span class="c7">&nbsp;topic. However, the </span><span class="c2">/stereo/depth</span><span class="c7">&nbsp;topic was not outputting any data, causing the </span><span class="c2">depth_extraction</span><span class="c7">&nbsp;node to crash. Despite this setback, the </span><span class="c2">controller</span><span class="c7">&nbsp;node was modified to infer depth using the bounding box, and publishing the data using a ROS topic to which the </span><span class="c2">controller </span><span class="c7">node will subscribe.</span></p><p class="c9"><span class="c10 c13"></span></p><h1 class="c12" id="h.qfojr3i4tcsm"><span class="c0">Project Goals </span></h1><p class="c16"><span class="c7">This project consists of creating a custom data set using the Turtlebot&rsquo;s OAK-D camera. The custom dataset is then used to train a model using the YOLOv5 [1] n</span><span class="c7">ano architecture</span><span class="c7">&nbsp;for object detection. In this case, the model was trained to detect an orange and a book. The trained model is then incorporated into a node, called </span><span class="c2">custom_object_detection_node</span><span class="c7">, that is used to extract the bounding box attributes of the detected object. Such attributes are the center point </span><span class="c7">x , center point y</span><span class="c7">, and the area of the bounding box. The extracted data is then published by the custom_object_detection_node node. Next, a node called controller subscribes to the topic being published by the custom_object_detection_node node. The data obtained by the controller node is used to write commands to the </span><span class="c2">/cmd_vel</span><span class="c7">&nbsp;topic and actuate the Turtlebot wheels. The controller node has a PID controller that is used on velocity commands of the Turtlebot to calculate the velocity based on the data received.</span></p><p class="c4"><span class="c10 c13 c27"></span></p><h2 class="c26 c29" id="h.lcsjysl8q8m6"><span class="c11">Initial objective</span><span class="c31 c11">&nbsp;</span></h2><p class="c16"><span class="c7">The overall goal of the project remained the same, to detect an object and extract information from the classified object to control the Turtlebot&rsquo;s velocity commands. The modification in the project objective was the way data was being processed to control the Turtlebot. Initially, the object was detected and the bounding box attributes were published. With the modification in place, </span><span class="c2">depth_extraction</span><span class="c7">&nbsp;node was created to subscribe to </span><span class="c2">custom_object_detection_node</span><span class="c7">&nbsp;node. The function of the </span><span class="c2">depth_extraction</span><span class="c7">&nbsp;node is to calculate the depth of the detected object using the bounding box attributes provided by the </span><span class="c2">custom_dataset_node</span><span class="c7">. A </span><span class="c2">controller </span><span class="c7">node would subscribe to the topic published by </span><span class="c2">depth_extraction</span><span class="c7">&nbsp;node to obtain the calculated depth. Simultaneously, the </span><span class="c2">controller </span><span class="c7">node was designed to write and publish commands to the </span><span class="c2">/cmd_vel</span><span class="c7">&nbsp;topic based on the received data. The </span><span class="c2">depth_extraction</span><span class="c7">&nbsp;node crashed every time the trained object was detected. It was determined that the </span><span class="c2">extraction_depth </span><span class="c7">node malfunction was caused by the </span><span class="c2">/stereo/depth</span><span class="c7">&nbsp;topic not outputting usable data. Figure 1 shows the data when echoing the </span><span class="c2">/stereo/depth</span><span class="c10 c7">&nbsp;topic. </span></p><p class="c4"><span class="c10 c13 c27"></span></p><p class="c6"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 626.61px; height: 424.72px;"><img alt="" src="images/image12.jpg" style="width: 626.61px; height: 424.72px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c6"><span class="c17">Figure 1. No Data Being Echoed via /stereo/depth Topic</span></p><p class="c4"><span class="c11 c19"></span></p><h1 class="c12" id="h.q3mg2oud2j8l"><span>Project WorkFlow</span></h1><p class="c26"><span class="c7">The process of the project did change because of the</span><span class="c2">&nbsp;/stereo_depth</span><span class="c7">&nbsp;topic publishing incorrect data. The figure below represents the updated project process.</span><span class="c10 c13">&nbsp;</span></p><p class="c4"><span class="c10 c13"></span></p><p class="c6"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 603.50px; height: 302.72px;"><img alt="" src="images/image4.jpg" style="width: 603.50px; height: 302.72px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c6"><span class="c8">Figure 2. Gantt chart</span></p><p class="c4"><span class="c19 c11"></span></p><h1 class="c12" id="h.ct56aacd3nt2"><span>Description of Project Process</span></h1><p class="c23"><span class="c10 c7">Initially, incorporating the LIDAR sensor into the project had become a possible idea to implement, but the idea was discarded due to time constraints. Scoping the project correctly was crucial to allow a successful completion of the proposed goal. Finally, it was determined to train a deep neural network for object detection and extract data from the classified object. The data would then be used to control the turtlebot. </span></p><p class="c30 c15"><span class="c3"></span></p><h2 class="c30 c29" id="h.dc6n5huodv25"><span>Data Collection</span></h2><p class="c23"><span class="c7">The first step for training the neural network is to determine the objects to be detected. </span><span class="c7">For collecting data, the OAK-D camera mounted on the robot is used. The data set is created using ROS bag recordings of around 2 minutes for each object, the object is placed in front of the robot at different angles. Around 2000 images for each class is collected in the initial ROS bag, which is pruned down to 200 images per class, selecting only clear and relevant images for annotations. The 400 images are uploaded on Roboflow, an online application used for annotations. After annotating the images, augmentation techniques were applied to increase the size of the dataset for better training and to prevent overfitting. To achieve this, the images were vertically and horizontally flipped, rotated by 45 degrees, adjusted saturation, brightness, exposure levels of the images, and induced noise. In the end, 1200 images are annotated per class for training a model. The dataset is split into three parts (Figure 3), train (70%), test (20%), and validation (10%), which is standard practice for training neural networks.</span></p><p class="c4"><span class="c3"></span></p><p class="c4"><span class="c3"></span></p><p class="c4"><span class="c3"></span></p><p class="c6"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 618.50px; height: 304.28px;"><img alt="" src="images/image11.png" style="width: 618.50px; height: 304.28px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c6"><span class="c8">Figure 3. The dataset Split into Train, Test, and Validate [2]</span></p><p class="c6 c15"><span class="c8"></span></p><p class="c6"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 622.57px; height: 307.50px;"><img alt="" src="images/image7.png" style="width: 622.57px; height: 307.50px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c6"><span class="c8">Figure 4. Training Charts for Roboflow Model [2]</span></p><p class="c6 c15"><span class="c8"></span></p><p class="c6"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 631.36px; height: 318.10px;"><img alt="" src="images/image8.png" style="width: 631.36px; height: 318.10px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c6"><span class="c8">Figure 5. Sample Inference with Predictions [2]</span></p><p class="c4"><span class="c5"></span></p><h2 class="c26 c29" id="h.397ddajxskgp"><span class="c5">Training using TensorFlow</span></h2><p class="c16"><span class="c10 c7">Initially, Roboflow was used to annotate the dataset. Roboflow also enables users to train and deploy a model online using its API [2]. This process took around two days for annotating the data, training and deploying the model on the robot using API. However, the training of the model using the custom dataset was used as a trial of how good the dataset is in training it on a deployable model on the robot without using the API. </span></p><p class="c9"><span class="c10 c7"></span></p><p class="c16"><span class="c10 c7">As discussed, the results of the trained model prove the dataset to be adequate. The dataset allows enough training and testing images to fine-tune and train a model. The deployment of the trained model on the robot was considered rather than deployment on the Ubuntu virtual machines and using it to infer and make the robot act accordingly. So a tensorflow lite model Efficient-det-lite-0 is selected. Initially, the Efficient-det-lite-0 results turned out poorly, and the model was taking up almost all the processing power of the robot [3]. Hence, a Tensorflow model zoo (SSD-Mobilenet-V2) was converted to a tensorflow-lite model to be deployed on the robot [4]. The results could have been better, yet, the processing power was much more, and took much less time to infer a single result. Plus, support for TensorFlow has been deprecated, so a faster, newer, and computationally less intensive solution is better.</span></p><p class="c4"><span class="c10 c7"></span></p><h2 class="c26 c29" id="h.glggjlisogk9"><span class="c5">Training using PyTorch</span></h2><p class="c16"><span class="c7">Next, the YOLOv5 model is trained on the dataset. </span><span class="c7">The trained model is converted to an open neural network exchange (ONNX) file to be deployed directly to the camera</span><span class="c10 c7">. The Oak-D camera has shave tensor processing units that can be used for image processing and computations. Hence, the robot would have a lighter computational burden, as these processes take place in the real time domain. However, the model was not deployed on the Oak-D camera as a pipeline for extracting images after killing the ROS node was required. The incorporation of a real time pipeline for this process inside a ROS node requires network level analysis.</span></p><p class="c4"><span class="c10 c13"></span></p><p class="c16"><span class="c10 c7">Finally, the model was trained using YOLOv5s [1], a smaller version of YOLOv5. YOLOv5s uses fewer parameters, hence, requires less computational power. The model was trained on Google Collab with the dataset at 150 epochs. The results of the trained model were satisfactory, exhibiting 1-5 FPS. However, 1-5 FPS is too slow for an application that needs to run in real-time, therefore, YOLOv5n was used next. YOLOv5s is the nano version of YOLOv5 with around 1.7 million parameters. The parameters are six times less than YOLOv5s and ten times less than YOLOv5. After deploying the model, the inferences were around 15 FPS and considered too slow for the real-time application. There are several ways of optimizing models, two techniques commonly used are pruning and quantization.</span></p><p class="c4"><span class="c10 c7"></span></p><p class="c26"><span class="c10 c7">Pruning trims the model while it is being trained, a method which requires a deeper understanding of the field. Quantization was incorporated as well, drastically improving the model&#39;s performance by converting the entire model to work on integer-type values instead of floating-point values. Working with floating point variables requires more space and time complexity than integer-type variables, making the model faster but slightly less efficient. After optimizations, the model was deployed &nbsp;on the robot at 25-30 FPS with a confidence interval of 0.4-0.9. Overall, there was a significant tradeoff between training the model to have a high confidence level or allowing the model to have a low inference time. </span></p><p class="c4"><span class="c1"></span></p><p class="c6"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 467.50px; height: 241.77px;"><img alt="" src="images/image10.png" style="width: 467.50px; height: 241.77px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c6"><span class="c8">Figure 6. Terminal Screenshot of the ROS Node Running</span></p><p class="c4"><span class="c19 c11"></span></p><h2 class="c12" id="h.3k9c87wsgsi4"><span>Validation</span></h2><p class="c16"><span class="c7">For an object detection model, the average confidence interval normally ranges between 0.6-0.8, which is an indication that the model is able to detect objects. </span><span class="c10 c7">As discussed above, different models were tested which returned inferences at different confidence levels. To validate if the model was satisfactory or not, &nbsp;an intuitive method for checking confidence levels, center point x, center point y and area of the bounding box of the object being detected at a different distance from the camera. If these inference attributes were consistent at different points where the object was being placed, it is considered the model was valid for deployment because even as a human, one is able to distinguish objects from surroundings at a certain confidence level depending on the position of the object, and one can consistently determine where the object lies.</span></p><p class="c4"><span class="c10 c13"></span></p><p class="c16"><span class="c7">After deploying the models on the robot, the model trained on Roboflow was the most accurate, with satisfactory inferences, and average confidence intervals between 0.6 and 0.9. The Efficient-Det-Lite-0 returned inferences with confidence interval 0.05-0.2 which is pretty low, </span><span class="c7 c33">YOLOv5</span><span class="c7">&nbsp;</span><span class="c7">and </span><span class="c7 c33">YOLOv5s</span><span class="c7">&nbsp;returned </span><span class="c7">inferences with confidence interval of 0.7-0.9, with slow processing time and so accuracy was traded for speed by using </span><span class="c7 c33">YOLOv5n</span><span class="c7">&nbsp;</span><span class="c10 c7">which had lesser accuracy but gave consistent results [3].</span></p><p class="c4"><span class="c10 c7"></span></p><h2 class="c12" id="h.oakf569ymsxz"><span class="c5">Controller Node</span></h2><p class="c16"><span class="c7">The </span><span class="c2">controller </span><span class="c7">node subscribes to the data published in </span><span class="c2">/object_center</span><span class="c7">. This topic contains the real time data (position and size of the boundary box) of the detected object. A PID class is defined in this node to control the velocity commands (published through </span><span class="c2">/cmd_vel</span><span class="c7">). In order for the robot to navigate in 2D space, two controller objects are defined for the angular z velocity command (yaw) and linear x velocity command (forward and backward). The input to these controllers are the data received in </span><span class="c2">/object_center</span><span class="c7">. To center the object, a setpoint of 125 is defined. This number signifies the horizontal center point of the 250 by 250 pixel image. The deduction of the x center point in the </span><span class="c2">/object_center </span><span class="c7">topic from the calibration setpoint, provides the error that is used in the PID calculations. The PID gains are defined as calibrations for this controller for future tuning. This PID controller controls the yaw command (angular velocity z) published in </span><span class="c2">/cmd_vel</span><span class="c7">. The forward and reverse velocity command is calculated by deduction of the received bounding area (in </span><span class="c2">/object_center</span><span class="c7 c10">) from the setpoint. This error is used in the PID controller for linear velocity x command. The closer the camera is to the image, the larger the boundary area. Hence, this controller controls the robot velocity for tracking the object. An additional saturation function is defined in the PID class that uses upper and lower limit velocity thresholds to limit the speed of the robot. The saturation function takes the PID output as the input argument, and based on the speed limit calibrations, if the value of PID output surpasses the thresholds, instead it will output the threshold value. This was done so the robot moves very smoothly without perfect PID tuning and unnecessary controller responses.</span></p><p class="c9"><span class="c10 c7"></span></p><h1 class="c12" id="h.7mtudrgm0xfq"><span>ROS </span><span>Architecture</span></h1><p class="c16"><span class="c7">The project in total has 2 additional nodes and 1 additional topic used to transfer information between the 2 nodes. As discussed we have </span><span class="c2">custom_object_detection_node </span><span class="c7">and</span><span class="c2">&nbsp;controller </span><span class="c10 c7">and /object_center topic.</span></p><p class="c9"><span class="c10 c7"></span></p><p class="c16"><span class="c7">The </span><span class="c2">custom_object_detection</span><span class="c7">&nbsp;node subscribes to </span><span class="c2">/color/preview/image</span><span class="c10 c7">&nbsp;topic and runs an instance of YOLOv5n object by loading the weights that are obtained after training. The weights are already pruned and quantization of the model takes place after loading the weights.</span></p><p class="c9"><span class="c10 c7"></span></p><p class="c16"><span class="c7">The </span><span class="c2">custom_object_detection_node </span><span class="c7">publishes the topic </span><span class="c2">/object_center</span><span class="c7">&nbsp;of type </span><span class="c2">Point</span><span class="c7">&nbsp;from </span><span class="c2">geometry_msg</span><span class="c7">&nbsp;which has 3 data fields: </span><span class="c2">x</span><span class="c7">, </span><span class="c2">y</span><span class="c7">&nbsp;and </span><span class="c2">z</span><span class="c7">. </span><span class="c2">Point.x</span><span class="c7">&nbsp;is the center point x, </span><span class="c2">Point.y</span><span class="c7">&nbsp;is the center point y and </span><span class="c2">Point.z</span><span class="c10 c7">&nbsp;is the area of the bounding box.</span></p><p class="c9"><span class="c10 c7"></span></p><p class="c16"><span class="c7">The controller node subscribes to the </span><span class="c2">/object_center</span><span class="c10 c7">&nbsp;topic to actuate the bot using the PID controller written inside the node.</span></p><p class="c16"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 300.00px;"><img alt="" src="images/image9.jpg" style="width: 624.00px; height: 300.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c6"><span class="c17">Figure 7. Project Architecture</span></p><p class="c9"><span class="c0"></span></p><h1 class="c12" id="h.rn95yyr2vzyl"><span>Trade-Offs</span></h1><p class="c16"><span class="c10 c7">Firstly, it was crucial to balance the need for accuracy with speed when selecting the model. Initially, Roboflow was used to train a model, which yielded accurate results with fair inferences. However, the model was not deployable on the robot, which led the project to try other options. Afterwards, a TensorFlow lite model was used, which needed to be more accurate and consume more processing power. Ultimately, &nbsp;the YOLOv5 method was used to train the dataset and convert it to an ONNX file that could be deployed directly to the Oak-D camera. Given the time constraint, YOLOv5 was the model training method to be used due to the accuracy and the reduced computational time on the robot</span></p><p class="c9"><span class="c10 c7"></span></p><p class="c16"><span class="c10 c7">Secondly, the balance model compatibility with computational resources needed to be taken into account. Initially, the TensorFlow lite model was used on the robot. However, high computational challenges arised due to the model&#39;s high processing power requirements. Therefore, PyTorch was selected to train a YOLOv5 model that could run directly on the Oak-D camera&#39;s tensor processing units, reducing the computational burden on the robot.</span></p><p class="c9"><span class="c10 c7"></span></p><p class="c16"><span class="c7">Finally, there was a &nbsp;trade-off between the ease of deployment and incorporating the model into the ROS node. Although deploying the ONNX file directly to the Oak-D camera would have been more comfortable, it was not feasible due to the need to set up a pipeline for extracting images after killing the ROS node. Therefore, the model was incorporated into a ROS node, a more complex task. The trade-offs consisted of &nbsp;balancing the completion of the goals or allowing the model to have good accuracy, being compatible, and taking into account the computational resources.</span></p><p class="c9"><span class="c1"></span></p><h1 class="c12" id="h.y0zvalub5eo7"><span>Videos</span></h1><p class="c4"><span class="c10 c20"></span></p><p class="c6"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 496.50px; height: 280.60px;"><img alt="" src="images/image5.png" style="width: 496.50px; height: 280.60px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c16"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 492.50px; height: 278.10px;"><img alt="" src="images/image2.png" style="width: 492.50px; height: 278.10px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c9"><span class="c19 c11"></span></p><hr style="page-break-before:always;display:none;"><p class="c9"><span class="c19 c28"></span></p><h1 class="c12" id="h.g9ga2d9ujt8i"><span>Citation</span></h1><p class="c4"><span class="c10 c20"></span></p><p class="c30"><span class="c7">[1] Ultralytics, &ldquo;GitHub - ultralytics/yolov5: YOLOv5 &#x1f680; in PyTorch &gt; ONNX &gt; CoreML &gt; TFLite,&rdquo; , Tag: v7.0, Commit: </span><span class="c7 c33 c35"><a class="c21" href="https://www.google.com/url?q=https://github.com/ultralytics/yolov5/commit/915bbf294bb74c859f0b41f1c23bc395014ea679&amp;sa=D&amp;source=editors&amp;ust=1683185492646730&amp;usg=AOvVaw0VeL0iyxKsLLr8mQiYTaF5">915bbf2</a></span><span class="c7 c25">, </span><span class="c7 c25">GitHub</span><span class="c7">. </span><span class="c22 c7"><a class="c21" href="https://www.google.com/url?q=https://github.com/ultralytics/yolov5&amp;sa=D&amp;source=editors&amp;ust=1683185492647351&amp;usg=AOvVaw3EzjAwT4TzMW1SswW-QrS9">https://github.com/ultralytics/yolov5</a></span></p><p class="c4"><span class="c10 c13"></span></p><p class="c30"><span class="c7">[2] Patel, P. (2023). </span><span class="c7 c25">Robdep Roboflow Workspace</span><span class="c7">. Roboflow. Retrieved from </span><span class="c7 c22"><a class="c21" href="https://www.google.com/url?q=https://app.roboflow.com/robdep&amp;sa=D&amp;source=editors&amp;ust=1683185492648212&amp;usg=AOvVaw14I8CcMyNtaPpLXcNHvVy9">https://app.roboflow.com/robdep</a></span></p><p class="c4"><span class="c10 c13"></span></p><p class="c30"><span class="c7">[3] &ldquo;Retrain EfficientDet-Lite detector for the Edge TPU (TF2)&rdquo; </span><span class="c22 c7"><a class="c21" href="https://www.google.com/url?q=https://colab.research.google.com/github/google-coral/tutorials/blob/master/retrain_efficientdet_model_maker_tf2.ipynb&amp;sa=D&amp;source=editors&amp;ust=1683185492648951&amp;usg=AOvVaw2a_3YibVUHRZBPfP2qcuiP">https://colab.research.google.com/github/google-coral/tutorials/blob/master/retrain_efficientdet_model_maker_tf2.ipynb</a></span></p><p class="c30 c15"><span class="c10 c7"></span></p><p class="c32"><span class="c7">[4] Tensorflow. (2021, May 7). </span><span class="c7 c25">TensorFlow Model Garden</span><span class="c7">. GitHub. Retrieved from </span><span class="c22 c7"><a class="c21" href="https://www.google.com/url?q=https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md&amp;sa=D&amp;source=editors&amp;ust=1683185492649697&amp;usg=AOvVaw2VEtXWJYx89Rfo6hSKbfLj">https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md</a></span></p><p class="c30 c15"><span class="c10 c7"></span></p><p class="c9"><span class="c10 c13"></span></p><hr style="page-break-before:always;display:none;"><p class="c9"><span class="c10 c13"></span></p><p class="c16"><span class="c0">Appendix</span></p><p class="c9"><span class="c5"></span></p><p class="c6"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 480.39px; height: 337.13px;"><img alt="" src="images/image6.png" style="width: 480.39px; height: 337.13px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c6"><span class="c17">Fig </span><span class="c8">8. Training Metrics for the YOLOv5n deployed on the robot.</span></p><p class="c9"><span class="c5"></span></p><p class="c6"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 288.00px; height: 412.59px;"><img alt="" src="images/image3.png" style="width: 288.00px; height: 412.59px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 288.00px; height: 412.76px;"><img alt="" src="images/image1.png" style="width: 288.00px; height: 412.76px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c6"><span class="c17">Fig 9. Training and Validation loss for the YOLOv5n deployed on the robot</span></p><div><p class="c4"><span class="c10 c20"></span></p></div></body></html>